package utils_test

import (
	"testing"

	"github.com/nyaruka/goflow/utils"

	"github.com/stretchr/testify/assert"
)

func TestSnakify(t *testing.T) {
	var snakeTests = []struct {
		input    string
		expected string
	}{
		{"Hello World", "hello_world"},
		{"hello_world", "hello_world"},
		{"hello-world", "hello_world"},
		{"hiğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜there", "hi_there"},
		{"æ˜¨å¤œã®ã‚³", "æ˜¨å¤œã®ã‚³"},
		{"this@isn't@email", "this_isn_t_email"},
	}

	for _, test := range snakeTests {
		assert.Equal(t, test.expected, utils.Snakify(test.input), "unexpected result snakifying '%s'", test.input)
	}
}

func TestURLEscape(t *testing.T) {
	var urlTests = []struct {
		input    string
		expected string
	}{
		{"", ""},
		{"hello_world-there", "hello_world-there"},
		{"foo: bar ? & some/thing", "foo%3A%20bar%20%3F%20%26%20some%2Fthing"},
	}

	for _, test := range urlTests {
		assert.Equal(t, test.expected, utils.URLEscape(test.input), "unexpected result URL escaping '%s'", test.input)
	}
}

func TestTokenizeString(t *testing.T) {
	tokenizerTests := []struct {
		text   string
		result []string
	}{
		{" one ", []string{"one"}},
		{"one   two three", []string{"one", "two", "three"}},
		{"one.two.three", []string{"one", "two", "three"}},
		{"O'Grady can't foo_bar", []string{"O'Grady", "can't", "foo_bar"}}, // single quotes and underscores don't split tokens
		{"Ã¶ne.Î²Î®Ï„Î±a.thÃ©", []string{"Ã¶ne", "Î²Î®Ï„Î±a", "thÃ©"}},                 // non-latin letters allowed in tokens
		{"  one(two!*@three ", []string{"one", "two", "three"}},            // other punctuation ignored
		{"spend$Â£â‚¬â‚ â‚£â‚ª", []string{"spend", "$", "Â£", "â‚¬", "â‚ ", "â‚£", "â‚ª"}},   // currency symbols treated as individual tokens
		{"math+=Ã—Ã·âˆšâˆŠ", []string{"math", "+", "=", "Ã—", "Ã·", "âˆš", "âˆŠ"}},     // math symbols treated as individual tokens
		{"emojiğŸ˜„ğŸ¥ğŸ‘ªğŸ‘°ğŸ˜ŸğŸ§Ÿ", []string{"emoji", "ğŸ˜„", "ğŸ¥", "ğŸ‘ª", "ğŸ‘°", "ğŸ˜Ÿ", "ğŸ§Ÿ"}},   // emojis treated as individual tokens
		{"ğŸ‘ğŸ¿ ğŸ‘¨ğŸ¼", []string{"ğŸ‘", "ğŸ¿", "ğŸ‘¨", "ğŸ¼"}},                            // tone modifiers treated as individual tokens
		{"â„¹ï¸ â„¹ï¸", []string{"â„¹", "â„¹"}},                                      // variation selectors ignored
	}
	for _, test := range tokenizerTests {
		assert.Equal(t, test.result, utils.TokenizeString(test.text), "unexpected result tokenizing '%s'", test.text)
	}
}

func TestTokenizeStringByChars(t *testing.T) {
	tokenizerTests := []struct {
		text   string
		chars  string
		result []string
	}{
		{"one   two three", " ", []string{"one", "two", "three"}},
		{"Jim O'Grady", " ", []string{"Jim", "O'Grady"}},
		{"one.Î²Î®Ï„Î±a/three", "./", []string{"one", "Î²Î®Ï„Î±a", "three"}},
		{"oneğŸ˜„three", "ğŸ˜„", []string{"one", "three"}},
		{"  one.two.*@three ", " .*@", []string{"one", "two", "three"}},
		{" one ", " ", []string{"one"}},
	}
	for _, test := range tokenizerTests {
		assert.Equal(t, test.result, utils.TokenizeStringByChars(test.text, test.chars), "unexpected result tokenizing '%s'", test.text)
	}
}
