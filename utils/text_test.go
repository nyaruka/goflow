package utils_test

import (
	"testing"

	"github.com/nyaruka/goflow/utils"

	"github.com/stretchr/testify/assert"
)

func TestSnakify(t *testing.T) {
	var snakeTests = []struct {
		input    string
		expected string
	}{
		{"Hello World", "hello_world"},
		{"hello_world", "hello_world"},
		{"hello-world", "hello_world"},
		{"_Hello World", "_hello_world"},
		{"   Hello World    ", "hello_world"},
		{"hiğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜there", "hi_there"},
		{"æ˜¨å¤œã®ã‚³", "æ˜¨å¤œã®ã‚³"},
		{"this@isn't@email", "this_isn_t_email"},
	}

	for _, test := range snakeTests {
		assert.Equal(t, test.expected, utils.Snakify(test.input), "unexpected result snakifying '%s'", test.input)
	}
}

func TestTokenizeString(t *testing.T) {
	tokenizerTests := []struct {
		text   string
		result []string
	}{
		{" one ", []string{"one"}},
		{"one   two three", []string{"one", "two", "three"}},
		{"one.two.three", []string{"one", "two", "three"}},
		{"O'Grady can't foo_bar", []string{"O'Grady", "can't", "foo_bar"}}, // single quotes and underscores don't split tokens
		{"Ã¶ne.Î²Î®Ï„Î±a.thÃ©", []string{"Ã¶ne", "Î²Î®Ï„Î±a", "thÃ©"}},                 // non-latin letters allowed in tokens
		{"ÙˆØ§Ø­Ø¯ Ø§Ø«Ù†ÙŠÙ† Ø«Ù„Ø§Ø«Ø©", []string{"ÙˆØ§Ø­Ø¯", "Ø§Ø«Ù†ÙŠÙ†", "Ø«Ù„Ø§Ø«Ø©"}},           // RTL scripts
		{"  \t\none(two!*@three ", []string{"one", "two", "three"}},        // other punctuation ignored
		{"spend$Â£â‚¬â‚ â‚£â‚ª", []string{"spend", "$", "Â£", "â‚¬", "â‚ ", "â‚£", "â‚ª"}},   // currency symbols treated as individual tokens
		{"math+=Ã—Ã·âˆšâˆŠ", []string{"math", "+", "=", "Ã—", "Ã·", "âˆš", "âˆŠ"}},     // math symbols treated as individual tokens
		{"emojiğŸ˜„ğŸ¥ğŸ‘ªğŸ‘°ğŸ˜ŸğŸ§Ÿ", []string{"emoji", "ğŸ˜„", "ğŸ¥", "ğŸ‘ª", "ğŸ‘°", "ğŸ˜Ÿ", "ğŸ§Ÿ"}},   // emojis treated as individual tokens
		{"ğŸ‘ğŸ¿ ğŸ‘¨ğŸ¼", []string{"ğŸ‘", "ğŸ¿", "ğŸ‘¨", "ğŸ¼"}},                            // tone modifiers treated as individual tokens
		{"â„¹ â„¹ï¸", []string{"â„¹", "â„¹ï¸"}},                                      // variation selectors ignored
		{"à¸¢à¸à¹€à¸¥à¸´à¸ sasa", []string{"à¸¢à¸à¹€à¸¥à¸´à¸", "sasa"}},                        // Thai word means Cancelled
		{"à¦¬à¦¾à¦¤à¦¿à¦² sasa", []string{"à¦¬à¦¾à¦¤à¦¿à¦²", "sasa"}},                          // Bangla word means Cancel
		{"á€‘á€½á€€á€ºá€á€½á€¬á€¸ sasa", []string{"á€‘á€½á€€á€ºá€á€½á€¬á€¸", "sasa"}},                    // Burmese word means exit
	}
	for _, test := range tokenizerTests {
		assert.Equal(t, test.result, utils.TokenizeString(test.text), "unexpected result tokenizing '%s'", test.text)
	}
}

func TestTokenizeStringByChars(t *testing.T) {
	tokenizerTests := []struct {
		text   string
		chars  string
		result []string
	}{
		{"one   two three", " ", []string{"one", "two", "three"}},
		{"Jim O'Grady", " ", []string{"Jim", "O'Grady"}},
		{"one.Î²Î®Ï„Î±a/three", "./", []string{"one", "Î²Î®Ï„Î±a", "three"}},
		{"oneğŸ˜„three", "ğŸ˜„", []string{"one", "three"}},
		{"  one.two.*@three ", " .*@", []string{"one", "two", "three"}},
		{" one ", " ", []string{"one"}},
	}
	for _, test := range tokenizerTests {
		assert.Equal(t, test.result, utils.TokenizeStringByChars(test.text, test.chars), "unexpected result tokenizing '%s'", test.text)
	}
}

func TestTokenizeStringByUnicodeSeg(t *testing.T) {
	tokenizerTests := []struct {
		text   string
		result []string
	}{
		// test cases taken from Apache Lucene (TestStandardAnalyzer.java) to assert equivalency
		{"ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ« 13 Õ´Õ«Õ¬Õ«Õ¸Õ¶ Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨ (4,600` Õ°Õ¡ÕµÕ¥Ö€Õ¥Õ¶ Õ¾Õ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ¸Ö‚Õ´) Õ£Ö€Õ¾Õ¥Õ¬ Õ¥Õ¶ Õ¯Õ¡Õ´Õ¡Õ¾Õ¸Ö€Õ¶Õ¥Ö€Õ« Õ¯Õ¸Õ²Õ´Õ«Ö Õ¸Ö‚ Õ°Õ¡Õ´Õ¡Ö€ÕµÕ¡ Õ¢Õ¸Õ¬Õ¸Ö€ Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨ Õ¯Õ¡Ö€Õ¸Õ² Õ§ Õ­Õ´Õ¢Õ¡Õ£Ö€Õ¥Õ¬ ÖÕ¡Õ¶Õ¯Õ¡Ö Õ´Õ¡Ö€Õ¤ Õ¸Õ¾ Õ¯Õ¡Ö€Õ¸Õ² Õ§ Õ¢Õ¡ÖÕ¥Õ¬ ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ« Õ¯Õ¡ÕµÖ„Õ¨Ö‰", []string{"ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ«", "13", "Õ´Õ«Õ¬Õ«Õ¸Õ¶", "Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨", "4,600", "Õ°Õ¡ÕµÕ¥Ö€Õ¥Õ¶", "Õ¾Õ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ¸Ö‚Õ´", "Õ£Ö€Õ¾Õ¥Õ¬", "Õ¥Õ¶", "Õ¯Õ¡Õ´Õ¡Õ¾Õ¸Ö€Õ¶Õ¥Ö€Õ«", "Õ¯Õ¸Õ²Õ´Õ«Ö", "Õ¸Ö‚", "Õ°Õ¡Õ´Õ¡Ö€ÕµÕ¡", "Õ¢Õ¸Õ¬Õ¸Ö€", "Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨", "Õ¯Õ¡Ö€Õ¸Õ²", "Õ§", "Õ­Õ´Õ¢Õ¡Õ£Ö€Õ¥Õ¬", "ÖÕ¡Õ¶Õ¯Õ¡Ö", "Õ´Õ¡Ö€Õ¤", "Õ¸Õ¾", "Õ¯Õ¡Ö€Õ¸Õ²", "Õ§", "Õ¢Õ¡ÖÕ¥Õ¬", "ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ«", "Õ¯Õ¡ÕµÖ„Õ¨"}},
		{"á‹ŠáŠªá”á‹µá‹« á‹¨á‰£áˆˆ á‰¥á‹™ á‰‹áŠ•á‰‹ á‹¨á‰°áˆŸáˆ‹ á‰µáŠ­áŠ­áˆˆáŠ›áŠ“ áŠáŒ» áˆ˜á‹áŒˆá‰  á‹•á‹á‰€á‰µ (áŠ¢áŠ•áˆ³á‹­áŠ­áˆá’á‹²á‹«) áŠá‹á¢ áˆ›áŠ•áŠ›á‹áˆ", []string{"á‹ŠáŠªá”á‹µá‹«", "á‹¨á‰£áˆˆ", "á‰¥á‹™", "á‰‹áŠ•á‰‹", "á‹¨á‰°áˆŸáˆ‹", "á‰µáŠ­áŠ­áˆˆáŠ›áŠ“", "áŠáŒ»", "áˆ˜á‹áŒˆá‰ ", "á‹•á‹á‰€á‰µ", "áŠ¢áŠ•áˆ³á‹­áŠ­áˆá’á‹²á‹«", "áŠá‹", "áˆ›áŠ•áŠ›á‹áˆ"}},
		{"Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ø§Ù„Ø£ÙˆÙ„ Ø¹Ù† ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ ÙŠØ³Ù…Ù‰ \"Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù…: Ù‚ØµØ© ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§\" (Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: Truth in Numbers: The Wikipedia Story)ØŒ Ø³ÙŠØªÙ… Ø¥Ø·Ù„Ø§Ù‚Ù‡ ÙÙŠ 2008.", []string{"Ø§Ù„ÙÙŠÙ„Ù…", "Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ", "Ø§Ù„Ø£ÙˆÙ„", "Ø¹Ù†", "ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§", "ÙŠØ³Ù…Ù‰", "Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©", "Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù…", "Ù‚ØµØ©", "ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§", "Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©", "Truth", "in", "Numbers", "The", "Wikipedia", "Story", "Ø³ÙŠØªÙ…", "Ø¥Ø·Ù„Ø§Ù‚Ù‡", "ÙÙŠ", "2008"}},
		{"Ü˜ÜÜ©ÜÜ¦Ü•ÜÜ (ÜÜ¢Ü“Ü ÜÜ: Wikipedia) Ü—Ü˜ ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ ÜšÜÜªÜ¬Ü Ü•ÜÜ¢Ü›ÜªÜ¢Ü› Ü’Ü Ü«Ü¢ÌˆÜ Ü£Ü“ÜÜÌˆÜÜ‚ Ü«Ü¡Ü— ÜÜ¬Ü Ü¡Ü¢ Ü¡ÌˆÜ Ü¬Ü Ü•\"Ü˜ÜÜ©Ü\" Ü˜\"ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ\"Ü€", []string{"Ü˜ÜÜ©ÜÜ¦Ü•ÜÜ", "ÜÜ¢Ü“Ü ÜÜ", "Wikipedia", "Ü—Ü˜", "ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ", "ÜšÜÜªÜ¬Ü", "Ü•ÜÜ¢Ü›ÜªÜ¢Ü›", "Ü’Ü Ü«Ü¢ÌˆÜ", "Ü£Ü“ÜÜÌˆÜ", "Ü«Ü¡Ü—", "ÜÜ¬Ü", "Ü¡Ü¢", "Ü¡ÌˆÜ Ü¬Ü", "Ü•", "Ü˜ÜÜ©Ü", "Ü˜", "ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ"}},
		{"à¦à¦‡ à¦¬à¦¿à¦¶à§à¦¬à¦•à§‹à¦· à¦ªà¦°à¦¿à¦šà¦¾à¦²à¦¨à¦¾ à¦•à¦°à§‡ à¦‰à¦‡à¦•à¦¿à¦®à¦¿à¦¡à¦¿à¦¯à¦¼à¦¾ à¦«à¦¾à¦‰à¦¨à§à¦¡à§‡à¦¶à¦¨ (à¦à¦•à¦Ÿà¦¿ à¦…à¦²à¦¾à¦­à¦œà¦¨à¦• à¦¸à¦‚à¦¸à§à¦¥à¦¾)à¥¤ à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾à¦° à¦¶à§à¦°à§ à§§à§« à¦œà¦¾à¦¨à§à¦¯à¦¼à¦¾à¦°à¦¿, à§¨à§¦à§¦à§§ à¦¸à¦¾à¦²à§‡à¥¤ à¦à¦–à¦¨ à¦ªà¦°à§à¦¯à¦¨à§à¦¤ à§¨à§¦à§¦à¦Ÿà¦¿à¦°à¦“ à¦¬à§‡à¦¶à§€ à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾ à¦°à¦¯à¦¼à§‡à¦›à§‡à¥¤", []string{"à¦à¦‡", "à¦¬à¦¿à¦¶à§à¦¬à¦•à§‹à¦·", "à¦ªà¦°à¦¿à¦šà¦¾à¦²à¦¨à¦¾", "à¦•à¦°à§‡", "à¦‰à¦‡à¦•à¦¿à¦®à¦¿à¦¡à¦¿à¦¯à¦¼à¦¾", "à¦«à¦¾à¦‰à¦¨à§à¦¡à§‡à¦¶à¦¨", "à¦à¦•à¦Ÿà¦¿", "à¦…à¦²à¦¾à¦­à¦œà¦¨à¦•", "à¦¸à¦‚à¦¸à§à¦¥à¦¾", "à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾à¦°", "à¦¶à§à¦°à§", "à§§à§«", "à¦œà¦¾à¦¨à§à¦¯à¦¼à¦¾à¦°à¦¿", "à§¨à§¦à§¦à§§", "à¦¸à¦¾à¦²à§‡", "à¦à¦–à¦¨", "à¦ªà¦°à§à¦¯à¦¨à§à¦¤", "à§¨à§¦à§¦à¦Ÿà¦¿à¦°à¦“", "à¦¬à§‡à¦¶à§€", "à¦­à¦¾à¦·à¦¾à¦¯à¦¼", "à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾", "à¦°à¦¯à¦¼à§‡à¦›à§‡"}},
		{"ÙˆÛŒÚ©ÛŒ Ù¾Ø¯ÛŒØ§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¯Ø± ØªØ§Ø±ÛŒØ® Û²Ûµ Ø¯ÛŒ Û±Û³Û·Û¹ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ú©Ù…Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ø´Ù†Ø§Ù…Ù‡Ù” ØªØ®ØµØµÛŒ Ù†ÙˆÙ¾Ø¯ÛŒØ§ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯.", []string{"ÙˆÛŒÚ©ÛŒ", "Ù¾Ø¯ÛŒØ§ÛŒ", "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ", "Ø¯Ø±", "ØªØ§Ø±ÛŒØ®", "Û²Ûµ", "Ø¯ÛŒ", "Û±Û³Û·Û¹", "Ø¨Ù‡", "ØµÙˆØ±Øª", "Ù…Ú©Ù…Ù„ÛŒ", "Ø¨Ø±Ø§ÛŒ", "Ø¯Ø§Ù†Ø´Ù†Ø§Ù…Ù‡Ù”", "ØªØ®ØµØµÛŒ", "Ù†ÙˆÙ¾Ø¯ÛŒØ§", "Ù†ÙˆØ´ØªÙ‡", "Ø´Ø¯"}},
		{"Î“ÏÎ¬Ï†ÎµÏ„Î±Î¹ ÏƒÎµ ÏƒÏ…Î½ÎµÏÎ³Î±ÏƒÎ¯Î± Î±Ï€ÏŒ ÎµÎ¸ÎµÎ»Î¿Î½Ï„Î­Ï‚ Î¼Îµ Ï„Î¿ Î»Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÏŒ wiki, ÎºÎ¬Ï„Î¹ Ï€Î¿Ï… ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ ÏŒÏ„Î¹ Î¬ÏÎ¸ÏÎ± Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï€ÏÎ¿ÏƒÏ„ÎµÎ¸Î¿ÏÎ½ Î® Î½Î± Î±Î»Î»Î¬Î¾Î¿Ï…Î½ Î±Ï€ÏŒ Ï„Î¿Î½ ÎºÎ±Î¸Î­Î½Î±.", []string{"Î“ÏÎ¬Ï†ÎµÏ„Î±Î¹", "ÏƒÎµ", "ÏƒÏ…Î½ÎµÏÎ³Î±ÏƒÎ¯Î±", "Î±Ï€ÏŒ", "ÎµÎ¸ÎµÎ»Î¿Î½Ï„Î­Ï‚", "Î¼Îµ", "Ï„Î¿", "Î»Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÏŒ", "wiki", "ÎºÎ¬Ï„Î¹", "Ï€Î¿Ï…", "ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹", "ÏŒÏ„Î¹", "Î¬ÏÎ¸ÏÎ±", "Î¼Ï€Î¿ÏÎµÎ¯", "Î½Î±", "Ï€ÏÎ¿ÏƒÏ„ÎµÎ¸Î¿ÏÎ½", "Î®", "Î½Î±", "Î±Î»Î»Î¬Î¾Î¿Ï…Î½", "Î±Ï€ÏŒ", "Ï„Î¿Î½", "ÎºÎ±Î¸Î­Î½Î±"}},
		//{"æˆ‘æ˜¯ä¸­å›½äººã€‚ ï¼‘ï¼’ï¼“ï¼” ï¼´ï½…ï½“ï½”ï½“ ", []string{"æˆ‘", "æ˜¯", "ä¸­", "å›½", "äºº", "ï¼‘ï¼’ï¼“ï¼”", "ï¼´ï½…ï½“ï½”ï½“"}},  // gives different result
		{"", []string{}},
		{".", []string{}},
		{" ", []string{}},
		{"B2B", []string{"B2B"}},
		{"2B", []string{"2B"}},
		{"some-dashed-phrase", []string{"some", "dashed", "phrase"}},
		{"dogs,chase,cats", []string{"dogs", "chase", "cats"}},
		{"ac/dc", []string{"ac", "dc"}},
		{"O'Reilly", []string{"O'Reilly"}},
		{"you're", []string{"you're"}},
		{"she's", []string{"she's"}},
		{"Jim's", []string{"Jim's"}},
		{"don't", []string{"don't"}},
		{"O'Reilly's", []string{"O'Reilly's"}},
		{"21.35", []string{"21.35"}},
		{"R2D2 C3PO", []string{"R2D2", "C3PO"}},
		{"216.239.63.104", []string{"216.239.63.104"}},
		{"216.239.63.104", []string{"216.239.63.104"}},
		{"David has 5000 bones", []string{"David", "has", "5000", "bones"}},
		{"C embedded developers wanted", []string{"C", "embedded", "developers", "wanted"}},
		{"foo bar FOO BAR", []string{"foo", "bar", "FOO", "BAR"}},
		{"foo      bar .  FOO <> BAR", []string{"foo", "bar", "FOO", "BAR"}},
		{"\"QUOTED\" word", []string{"QUOTED", "word"}},
		{"ì•ˆë…•í•˜ì„¸ìš” í•œê¸€ì…ë‹ˆë‹¤", []string{"ì•ˆë…•í•˜ì„¸ìš”", "í•œê¸€ì…ë‹ˆë‹¤"}},
	}
	for _, test := range tokenizerTests {
		assert.Equal(t, test.result, utils.TokenizeStringByUnicodeSeg(test.text), "unexpected result tokenizing '%s'", test.text)
	}
}

func TestPrefixOverlap(t *testing.T) {
	assert.Equal(t, 0, utils.PrefixOverlap("", ""))
	assert.Equal(t, 0, utils.PrefixOverlap("abc", ""))
	assert.Equal(t, 0, utils.PrefixOverlap("", "abc"))
	assert.Equal(t, 0, utils.PrefixOverlap("a", "x"))
	assert.Equal(t, 1, utils.PrefixOverlap("x", "x"))
	assert.Equal(t, 3, utils.PrefixOverlap("xyz", "xyz"))
	assert.Equal(t, 2, utils.PrefixOverlap("xya", "xyz"))
	assert.Equal(t, 2, utils.PrefixOverlap("ğŸ˜„ğŸ˜ŸğŸ‘¨ğŸ¼", "ğŸ˜„ğŸ˜ŸğŸ‘°"))
	assert.Equal(t, 4, utils.PrefixOverlap("25078", "25073254252"))
}

func TestStringSlices(t *testing.T) {
	assert.Equal(t, []string{"he", "hello", "world"}, utils.StringSlices("hello world", []int{0, 2, 0, 5, 6, 11}))
}

func TestStringSliceContains(t *testing.T) {
	assert.False(t, utils.StringSliceContains(nil, "a", true))
	assert.False(t, utils.StringSliceContains([]string{}, "a", true))
	assert.False(t, utils.StringSliceContains([]string{"b", "c"}, "a", true))
	assert.True(t, utils.StringSliceContains([]string{"b", "a", "c"}, "a", true))
	assert.False(t, utils.StringSliceContains([]string{"b", "a", "c"}, "A", true))
	assert.True(t, utils.StringSliceContains([]string{"b", "a", "c"}, "A", false))
}

func TestIndent(t *testing.T) {
	assert.Equal(t, "", utils.Indent("", "  "))
	assert.Equal(t, "  x", utils.Indent("x", "  "))
	assert.Equal(t, "  x\n  y", utils.Indent("x\ny", "  "))
	assert.Equal(t, "  x\n\n  y", utils.Indent("x\n\ny", "  "))
	assert.Equal(t, ">>>x", utils.Indent("x", ">>>"))
}

func TestStringSet(t *testing.T) {
	assert.Equal(t, map[string]bool{}, utils.StringSet(nil))
	assert.Equal(t, map[string]bool{}, utils.StringSet([]string{}))
	assert.Equal(t, map[string]bool{"x": true, "y": true, "a": true}, utils.StringSet([]string{"a", "x", "y"}))
}

func TestStringSetKeys(t *testing.T) {
	assert.Equal(t, []string{}, utils.StringSetKeys(map[string]bool{}))
	assert.Equal(t, []string{"a", "x", "y"}, utils.StringSetKeys(map[string]bool{"x": true, "y": true, "a": true}))
}

func TestTruncateEllipsis(t *testing.T) {
	assert.Equal(t, "", utils.TruncateEllipsis("", 100))
	assert.Equal(t, "1234567890", utils.TruncateEllipsis("1234567890", 100))
	assert.Equal(t, "1234567890", utils.TruncateEllipsis("1234567890", 10))
	assert.Equal(t, "1234...", utils.TruncateEllipsis("1234567890", 7))
	assert.Equal(t, "ä½ å–œæ¬¢æˆ‘å½“ç„¶å–œæ¬¢çš„ç”µ", utils.TruncateEllipsis("ä½ å–œæ¬¢æˆ‘å½“ç„¶å–œæ¬¢çš„ç”µ", 100))
	assert.Equal(t, "ä½ å–œæ¬¢æˆ‘å½“ç„¶å–œæ¬¢çš„ç”µ", utils.TruncateEllipsis("ä½ å–œæ¬¢æˆ‘å½“ç„¶å–œæ¬¢çš„ç”µ", 10))
	assert.Equal(t, "ä½ å–œæ¬¢æˆ‘...", utils.TruncateEllipsis("ä½ å–œæ¬¢æˆ‘å½“ç„¶å–œæ¬¢çš„ç”µ", 7))
}

func TestTruncate(t *testing.T) {
	assert.Equal(t, "", utils.Truncate("", 100))
	assert.Equal(t, "1234567890", utils.Truncate("1234567890", 100))
	assert.Equal(t, "1234567890", utils.Truncate("1234567890", 10))
	assert.Equal(t, "1234567", utils.Truncate("1234567890", 7))
	assert.Equal(t, "ä½ å–œæ¬¢æˆ‘å½“ç„¶å–œæ¬¢çš„ç”µ", utils.Truncate("ä½ å–œæ¬¢æˆ‘å½“ç„¶å–œæ¬¢çš„ç”µ", 100))
	assert.Equal(t, "ä½ å–œæ¬¢æˆ‘å½“ç„¶å–œæ¬¢çš„ç”µ", utils.Truncate("ä½ å–œæ¬¢æˆ‘å½“ç„¶å–œæ¬¢çš„ç”µ", 10))
	assert.Equal(t, "ä½ å–œæ¬¢æˆ‘å½“ç„¶å–œ", utils.Truncate("ä½ å–œæ¬¢æˆ‘å½“ç„¶å–œæ¬¢çš„ç”µ", 7))
}

func TestRedactor(t *testing.T) {
	assert.Equal(t, "hello world", utils.NewRedactor("****")("hello world"))                         // nothing to redact
	assert.Equal(t, "", utils.NewRedactor("****", "abc")(""))                                        // empty input
	assert.Equal(t, "**** def **** def", utils.NewRedactor("****", "abc")("abc def abc def"))        // all instances redacted
	assert.Equal(t, "**** def **** jkl", utils.NewRedactor("****", "abc", "ghi")("abc def ghi jkl")) // all values redacted
}
